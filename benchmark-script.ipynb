{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: please install the following packages, pip install flair, pip install allennlp\n",
    "\n",
    "#### The following embeddings are available for benchmark in this script: (1) USE, (2) BERT, (3) ELMo, (4) XLNet, (5) RoBERTa, (6) XLM, (7) GPT-2, (8) Transformer-XL, (9) Flair\n",
    "\n",
    "#### Reminder: -------------PLEASE RUN BERT-AS-A-SERVICE-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "from pytorch_transformers import *\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "from flair.embeddings import *\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Lambda, Input, Embedding\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA SET\n",
    "# THE FOLLOWING CODE TAKES IN A CSV WITH COLUMN 0 AS TEXT AND COLUMN 1 AS INTENT\n",
    "data = pd.read_csv('path to csv file')\n",
    "intent = list(data[\"intent\"])\n",
    "text = list(data[\"text\"])\n",
    "\n",
    "# TRAIN-TEST SPLIT\n",
    "x_train, x_test, y_train, y_test = train_test_split(text, intent, test_size=0.2, random_state=0, stratify=intent)\n",
    "train_dict = {'text':x_train, 'intent':y_train}\n",
    "test_dict = {'text':x_test, 'intent':y_test}\n",
    "train = pd.DataFrame.from_dict(train_dict)\n",
    "test = pd.DataFrame.from_dict(test_dict)\n",
    "\n",
    "# DISPLAY DATA SET DISTRIBUTION\n",
    "print(\"Total Samples: \\t\\t\\t\", len(train)+len(test))\n",
    "print(\"Samples in training set: \\t\", len(train))\n",
    "print(\"Samples in testing set: \\t\", len(test))\n",
    "print(\"\\nNumber of intents: \\t\\t\", len(data.iloc[:,1].unique()))\n",
    "    \n",
    "print(\"\\nTraining distribution\\n------------------------\")\n",
    "print(train.iloc[:, 1].value_counts())\n",
    "print(\"\\nTesting distribution\\n-------------------------\")\n",
    "print(test.iloc[:, 1].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Universal():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.use = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "        self.n_labels = None\n",
    "    \n",
    "    def one_hot(self, label):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(label)\n",
    "        le_labels = le.transform(label)\n",
    "        ohe_labels = to_categorical(le_labels)\n",
    "        return ohe_labels\n",
    "        \n",
    "    def load_data(self, train, test):\n",
    "        self.train_x = train.iloc[:,0]\n",
    "        self.test_x = test.iloc[:,0]\n",
    "        self.train_y = self.one_hot(list(train.iloc[:,1]))\n",
    "        self.test_y = self.one_hot(list(test.iloc[:,1]))\n",
    "        self.n_labels = len(train.iloc[:,1].unique())\n",
    "\n",
    "    def embed(self, x):\n",
    "        return self.use(tf.squeeze(tf.cast(x, tf.string)))\n",
    "    \n",
    "    def model(self):\n",
    "        input_text = Input(shape=(1,), dtype='string')\n",
    "        embedding = Lambda(self.embed, output_shape=(512,))(input_text)\n",
    "        dense = Dense(256, activation='relu')(embedding)\n",
    "        dense = Dropout(0.3)(dense)\n",
    "        pred = Dense(self.n_labels, activation='softmax')(dense)\n",
    "        self.model = Model(inputs=[input_text], outputs=pred)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            ckpt = ModelCheckpoint('./model/use/{epoch:02d}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            history = self.model.fit(np.asarray(self.train_x), np.asarray(self.train_y), validation_split=0.1, epochs=20, batch_size=64)\n",
    "            self.model.save_weights('./model/use_model.h5')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        print(\"\\nEvaluating......\")\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            self.model.load_weights('./model/use_model.h5')\n",
    "            loss, acc = self.model.evaluate(np.asarray(self.test_x), np.asarray(self.test_y))\n",
    "            print(\"\\nThe loss is: \", loss, \"The accuracy is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = Universal()\n",
    "use.load_data(train, test)\n",
    "use.model()\n",
    "use.train()\n",
    "use.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bert = BertClient()\n",
    "        self.n_labels = None\n",
    "    \n",
    "    def one_hot(self, label):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(label)\n",
    "        le_labels = le.transform(label)\n",
    "        ohe_labels = to_categorical(le_labels)\n",
    "        return ohe_labels\n",
    "    \n",
    "    def load_data(self, train, test):\n",
    "        self.n_labels = len(train.iloc[:,1].unique())\n",
    "        \n",
    "        start = time.time()\n",
    "        self.train_x = self.bert.encode(list(train.iloc[:,0]))\n",
    "        end = time.time()\n",
    "        print(\"Time taken to encode train: \", end-start)\n",
    "        self.train_y = self.one_hot(train.iloc[:,1])\n",
    "        start = time.time()\n",
    "        self.test_x = self.bert.encode(list(test.iloc[:,0]))\n",
    "        end = time.time()\n",
    "        print(\"Time taken to encode test: \", end-start)\n",
    "        self.test_y = self.one_hot(test.iloc[:,1])\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(256, input_dim=1024, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(self.n_labels, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            ckpt = ModelCheckpoint('./model/bert/{epoch:02d}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            history = self.model.fit(np.asarray(self.train_x), np.asarray(self.train_y), validation_split=0.1, epochs=20, batch_size=64, callbacks=[ckpt])\n",
    "            self.model.save_weights('./model/bert_model.h5')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            self.model.load_weights('./model/bert_model.h5')\n",
    "            loss, acc = self.model.evaluate(np.asarray(self.test_x), np.asarray(self.test_y))\n",
    "            print(\"\\nThe loss is: \", loss, \"The accuracy is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = Bert()\n",
    "bert.load_data(train, test)\n",
    "bert.model()\n",
    "bert.train()\n",
    "bert.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELMo():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.elmo_embedding = ELMoEmbeddings('large')\n",
    "        self.document_embeddings = DocumentPoolEmbeddings([self.elmo_embedding])\n",
    "        self.n_labels = None\n",
    "    \n",
    "    def one_hot(self, label):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(label)\n",
    "        le_labels = le.transform(label)\n",
    "        ohe_labels = to_categorical(le_labels)\n",
    "        return ohe_labels\n",
    "        \n",
    "    def load_data(self, train, test):\n",
    "        self.n_labels = len(train.iloc[:,1].unique())\n",
    "        \n",
    "        self.train_x = []\n",
    "        self.test_x = []\n",
    "        \n",
    "        time.sleep(1)\n",
    "        for t in tqdm(train.iloc[:,0].to_list(), desc=\"Train data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.train_x.append(sentence.get_embedding().detach().tolist())\n",
    "\n",
    "        time.sleep(1)\n",
    "        for t in tqdm(test.iloc[:,0].to_list(), desc=\"Test data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.test_x.append(sentence.get_embedding().detach().tolist())\n",
    "        \n",
    "        self.train_y = self.one_hot(list(train.iloc[:,1]))\n",
    "        self.test_y = self.one_hot(list(test.iloc[:,1]))\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(256, input_dim=3072, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(self.n_labels, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            ckpt = ModelCheckpoint('./model/elmo/{epoch:02d}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            history = self.model.fit(np.asarray(self.train_x), np.asarray(self.train_y), validation_split=0.1, epochs=20, batch_size=64, callbacks=[ckpt])\n",
    "            self.model.save_weights('./model/elmo_model.h5')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            self.model.load_weights('./model/elmo_model.h5')\n",
    "            loss, acc = self.model.evaluate(np.asarray(self.test_x), np.asarray(self.test_y))\n",
    "            print(\"\\nThe loss is: \", loss, \"The accuracy is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "elmo = ELMo()\n",
    "elmo.load_data(train, test)\n",
    "elmo.model()\n",
    "elmo.train()\n",
    "elmo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNet():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.xlnet_embedding = XLNetEmbeddings(pooling_operation=\"mean\")\n",
    "        self.document_embeddings = DocumentPoolEmbeddings([self.xlnet_embedding])\n",
    "        self.n_labels = None\n",
    "    \n",
    "    def one_hot(self, label):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(label)\n",
    "        le_labels = le.transform(label)\n",
    "        ohe_labels = to_categorical(le_labels)\n",
    "        return ohe_labels\n",
    "        \n",
    "    def load_data(self, train, test):\n",
    "        self.n_labels = len(train.iloc[:,1].unique())\n",
    "        \n",
    "        self.train_x = []\n",
    "        self.test_x = []\n",
    "        \n",
    "        time.sleep(1)\n",
    "        for t in tqdm(train.iloc[:,0].to_list(), desc=\"Train data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.train_x.append(sentence.get_embedding().detach().tolist())\n",
    "\n",
    "        time.sleep(1)\n",
    "        for t in tqdm(test.iloc[:,0].to_list(), desc=\"Test data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.test_x.append(sentence.get_embedding().detach().tolist())\n",
    "        \n",
    "        self.train_y = self.one_hot(list(train.iloc[:,1]))\n",
    "        self.test_y = self.one_hot(list(test.iloc[:,1]))\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(256, input_dim=1024, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(self.n_labels, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            ckpt = ModelCheckpoint('./model/xlnet/{epoch:02d}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            history = self.model.fit(np.asarray(self.train_x), np.asarray(self.train_y), validation_split=0.1, epochs=20, batch_size=64, callbacks=[ckpt])\n",
    "            self.model.save_weights('./model/xlnet_model.h5')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            self.model.load_weights('./model/xlnet_model.h5')\n",
    "            loss, acc = self.model.evaluate(np.asarray(self.test_x), np.asarray(self.test_y))\n",
    "            print(\"\\nThe loss is: \", loss, \"The accuracy is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLnet = XLNet()\n",
    "XLnet.load_data(train, test)\n",
    "XLnet.model()\n",
    "XLnet.train()\n",
    "XLnet.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roberta():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.roberta_embedding = RoBERTaEmbeddings('roberta-large', layers=\"-2\", pooling_operation=\"mean\")\n",
    "        self.document_embeddings = DocumentPoolEmbeddings([self.roberta_embedding])\n",
    "        self.n_labels = None\n",
    "    \n",
    "    def one_hot(self, label):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(label)\n",
    "        le_labels = le.transform(label)\n",
    "        ohe_labels = to_categorical(le_labels)\n",
    "        return ohe_labels\n",
    "        \n",
    "    def load_data(self, train, test):\n",
    "        self.n_labels = len(train.iloc[:,1].unique())\n",
    "        \n",
    "        self.train_x = []\n",
    "        self.test_x = []\n",
    "        \n",
    "        time.sleep(1)\n",
    "        for t in tqdm(train.iloc[:,0].to_list(), desc=\"Train data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.train_x.append(sentence.get_embedding().detach().tolist())\n",
    "\n",
    "        time.sleep(1)\n",
    "        for t in tqdm(test.iloc[:,0].to_list(), desc=\"Test data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.test_x.append(sentence.get_embedding().detach().tolist())\n",
    "        \n",
    "        self.train_y = self.one_hot(list(train.iloc[:,1]))\n",
    "        self.test_y = self.one_hot(list(test.iloc[:,1]))\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(256, input_dim=1024, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(self.n_labels, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            ckpt = ModelCheckpoint('./model/roberta/{epoch:02d}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            history = self.model.fit(np.asarray(self.train_x), np.asarray(self.train_y), validation_split=0.1, epochs=20, batch_size=64, callbacks=[ckpt])\n",
    "            self.model.save_weights('./model/roberta_model.h5')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            self.model.load_weights('./model/roberta_model.h5')\n",
    "            loss, acc = self.model.evaluate(np.asarray(self.test_x), np.asarray(self.test_y))\n",
    "            print(\"\\nThe loss is: \", loss, \"The accuracy is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = Roberta()\n",
    "roberta.load_data(train, test)\n",
    "roberta.model()\n",
    "roberta.train()\n",
    "roberta.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLM():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.xlm_embedding = XLMEmbeddings(pooling_operation=\"mean\")\n",
    "        self.document_embeddings = DocumentPoolEmbeddings([self.xlm_embedding])\n",
    "        self.n_labels = None\n",
    "    \n",
    "    def one_hot(self, label):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(label)\n",
    "        le_labels = le.transform(label)\n",
    "        ohe_labels = to_categorical(le_labels)\n",
    "        return ohe_labels\n",
    "        \n",
    "    def load_data(self, train, test):\n",
    "        self.n_labels = len(train.iloc[:,1].unique())\n",
    "        \n",
    "        self.train_x = []\n",
    "        self.test_x = []\n",
    "        \n",
    "        time.sleep(1)\n",
    "        for t in tqdm(train.iloc[:,0].to_list(), desc=\"Train data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.train_x.append(sentence.get_embedding().detach().tolist())\n",
    "\n",
    "        time.sleep(1)\n",
    "        for t in tqdm(test.iloc[:,0].to_list(), desc=\"Test data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.test_x.append(sentence.get_embedding().detach().tolist())\n",
    "        \n",
    "        self.train_y = self.one_hot(list(train.iloc[:,1]))\n",
    "        self.test_y = self.one_hot(list(test.iloc[:,1]))\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(256, input_dim=2048, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(self.n_labels, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            ckpt = ModelCheckpoint('./model/xlm/{epoch:02d}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            history = self.model.fit(np.asarray(self.train_x), np.asarray(self.train_y), validation_split=0.1, epochs=20, batch_size=64, callbacks=[ckpt])\n",
    "            self.model.save_weights('./model/xlm_model.h5')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            self.model.load_weights('./model/xlm_model.h5')\n",
    "            loss, acc = self.model.evaluate(np.asarray(self.test_x), np.asarray(self.test_y))\n",
    "            print(\"\\nThe loss is: \", loss, \"The accuracy is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLM = XLM()\n",
    "XLM.load_data(train, test)\n",
    "XLM.model()\n",
    "XLM.train()\n",
    "XLM.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 - GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gpt2_embedding = OpenAIGPT2Embeddings()\n",
    "        self.document_embeddings = DocumentPoolEmbeddings([self.gpt2_embedding])\n",
    "        self.n_labels = None\n",
    "    \n",
    "    def one_hot(self, label):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(label)\n",
    "        le_labels = le.transform(label)\n",
    "        ohe_labels = to_categorical(le_labels)\n",
    "        return ohe_labels\n",
    "        \n",
    "    def load_data(self, train, test):\n",
    "        self.n_labels = len(train.iloc[:,1].unique())\n",
    "        \n",
    "        self.train_x = []\n",
    "        self.test_x = []\n",
    "        \n",
    "        time.sleep(1)\n",
    "        for t in tqdm(train.iloc[:,0].to_list(), desc=\"Train data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.train_x.append(sentence.get_embedding().detach().tolist())\n",
    "\n",
    "        time.sleep(1)\n",
    "        for t in tqdm(test.iloc[:,0].to_list(), desc=\"Test data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.test_x.append(sentence.get_embedding().detach().tolist())\n",
    "        \n",
    "        self.train_y = self.one_hot(list(train.iloc[:,1]))\n",
    "        self.test_y = self.one_hot(list(test.iloc[:,1]))\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(256, input_dim=2048, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(self.n_labels, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            ckpt = ModelCheckpoint('./model/gpt-2/{epoch:02d}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            history = self.model.fit(np.asarray(self.train_x), np.asarray(self.train_y), validation_split=0.1, epochs=20, batch_size=64, callbacks=[ckpt])\n",
    "            self.model.save_weights('./model/gpt2_model.h5')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            self.model.load_weights('./model/gpt2_model.h5')\n",
    "            loss, acc = self.model.evaluate(np.asarray(self.test_x), np.asarray(self.test_y))\n",
    "            print(\"\\nThe loss is: \", loss, \"The accuracy is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2 = GPT2()\n",
    "GPT2.load_data(train, test)\n",
    "GPT2.model()\n",
    "GPT2.train()\n",
    "GPT2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 - Transformer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransXL():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transxl_embedding = TransformerXLEmbeddings()\n",
    "        self.document_embeddings = DocumentPoolEmbeddings([self.transxl_embedding])\n",
    "        self.n_labels = None\n",
    "    \n",
    "    def one_hot(self, label):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(label)\n",
    "        le_labels = le.transform(label)\n",
    "        ohe_labels = to_categorical(le_labels)\n",
    "        return ohe_labels\n",
    "        \n",
    "    def load_data(self, train, test):\n",
    "        self.n_labels = len(train.iloc[:,1].unique())\n",
    "        \n",
    "        self.train_x = []\n",
    "        self.test_x = []\n",
    "        \n",
    "        time.sleep(1)\n",
    "        for t in tqdm(train.iloc[:,0].to_list(), desc=\"Train data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.train_x.append(sentence.get_embedding().detach().tolist())\n",
    "\n",
    "        time.sleep(1)\n",
    "        for t in tqdm(test.iloc[:,0].to_list(), desc=\"Test data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.test_x.append(sentence.get_embedding().detach().tolist())\n",
    "        \n",
    "        self.train_y = self.one_hot(list(train.iloc[:,1]))\n",
    "        self.test_y = self.one_hot(list(test.iloc[:,1]))\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(256, input_dim=3072, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(self.n_labels, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            ckpt = ModelCheckpoint('./model/transxl/{epoch:02d}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            history = self.model.fit(np.asarray(self.train_x), np.asarray(self.train_y), validation_split=0.1, epochs=20, batch_size=64, callbacks=[ckpt])\n",
    "            self.model.save_weights('./model/transxl_model.h5')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            self.model.load_weights('./model/transxl_model.h5')\n",
    "            loss, acc = self.model.evaluate(np.asarray(self.test_x), np.asarray(self.test_y))\n",
    "            print(\"\\nThe loss is: \", loss, \"The accuracy is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXL = TransXL()\n",
    "TXL.load_data(train, test)\n",
    "TXL.model()\n",
    "TXL.train()\n",
    "TXL.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 - Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flair():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.glove_embedding = WordEmbeddings('glove')\n",
    "        self.flair_embedding_forward = FlairEmbeddings('mix-forward')\n",
    "        self.flair_embedding_backward = FlairEmbeddings('mix-backward')\n",
    "        self.document_embeddings = DocumentPoolEmbeddings([self.glove_embedding,\n",
    "                                              self.flair_embedding_backward,\n",
    "                                              self.flair_embedding_forward])\n",
    "        self.n_labels = None\n",
    "        \n",
    "    def one_hot(self, label):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(label)\n",
    "        le_labels = le.transform(label)\n",
    "        ohe_labels = to_categorical(le_labels)\n",
    "        return ohe_labels\n",
    "        \n",
    "    def load_data(self, train, test):\n",
    "        self.n_labels = len(train.iloc[:,1].unique())\n",
    "        \n",
    "        self.train_x = []\n",
    "        self.test_x = []\n",
    "        \n",
    "        time.sleep(1)\n",
    "        for t in tqdm(train.iloc[:,0].to_list(), desc=\"Train data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.train_x.append(sentence.get_embedding().detach().tolist())\n",
    "\n",
    "        time.sleep(1)\n",
    "        for t in tqdm(test.iloc[:,0].to_list(), desc=\"Test data: \"):\n",
    "            sentence = Sentence(t)\n",
    "            self.document_embeddings.embed(sentence)\n",
    "            self.test_x.append(sentence.get_embedding().detach().tolist())\n",
    "        \n",
    "        self.train_y = self.one_hot(list(train.iloc[:,1]))\n",
    "        self.test_y = self.one_hot(list(test.iloc[:,1]))\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(256, input_dim=4196, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(self.n_labels, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            ckpt = ModelCheckpoint('./model/flair/{epoch:02d}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "            history = self.model.fit(np.asarray(self.train_x), np.asarray(self.train_y), validation_split=0.1, epochs=20, batch_size=64, callbacks=[ckpt])\n",
    "            self.model.save_weights('./model/flair_model.h5')\n",
    "            \n",
    "    def evaluate(self):\n",
    "        with tf.Session() as session:\n",
    "            K.set_session(session)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "            self.model.load_weights('./model/flair_model.h5')\n",
    "            loss, acc = self.model.evaluate(np.asarray(self.test_x), np.asarray(self.test_y))\n",
    "            print(\"\\nThe loss is: \", loss, \"The accuracy is: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flair = Flair()\n",
    "Flair.load_data(train, test)\n",
    "Flair.model()\n",
    "Flair.train()\n",
    "Flair.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
